{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to classify our datasets using multiple classification algorithms and evaluate their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn is not installed. Installing...\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\elodie\\desktop\\esiee\\ia\\pacman\\env\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\elodie\\desktop\\esiee\\ia\\pacman\\env\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\elodie\\desktop\\esiee\\ia\\pacman\\env\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\elodie\\desktop\\esiee\\ia\\pacman\\env\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\elodie\\desktop\\esiee\\ia\\pacman\\env\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "imblearn is already installed.\n",
      "deslib is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Packages to install\n",
    "packages_to_install = ['scikit-learn', 'imblearn', 'deslib']\n",
    "\n",
    "# Check if they are already installed\n",
    "import importlib\n",
    "for package in packages_to_install:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package} is not installed. Installing...\")\n",
    "        !pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>12792</td>\n",
       "      <td>12810</td>\n",
       "      <td>12809</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12724</td>\n",
       "      <td>12710</td>\n",
       "      <td>12792</td>\n",
       "      <td>12810</td>\n",
       "      <td>12809</td>\n",
       "      <td>12810</td>\n",
       "      <td>12810</td>\n",
       "      <td>12724</td>\n",
       "      <td>12717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0.590555</td>\n",
       "      <td>220.110174</td>\n",
       "      <td>21.125644</td>\n",
       "      <td>714.120693</td>\n",
       "      <td>3719.146472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.705523</td>\n",
       "      <td>0.476584</td>\n",
       "      <td>345.489585</td>\n",
       "      <td>8.802629</td>\n",
       "      <td>353.379868</td>\n",
       "      <td>679.766033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1          2           3            4         5   \\\n",
       "count      12810       12810      12810       12810        12810     12810   \n",
       "unique     12792       12810      12809       12810        12810     12724   \n",
       "top     0.590555  220.110174  21.125644  714.120693  3719.146472  0.000000   \n",
       "freq           2           1          2           1            1         6   \n",
       "\n",
       "              6         8           9         10          11          12  \\\n",
       "count      12810     12810       12810     12810       12810       12810   \n",
       "unique     12710     12792       12810     12809       12810       12810   \n",
       "top     0.705523  0.476584  345.489585  8.802629  353.379868  679.766033   \n",
       "freq           2         2           1         2           1           1   \n",
       "\n",
       "              13        14  \n",
       "count      12810     12810  \n",
       "unique     12724     12717  \n",
       "top     0.000000  0.425668  \n",
       "freq           6         2  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = pd.read_csv(\"./Ressources/Datasets/stand_norm_e1.txt\", header=None, delimiter=\" \")\n",
    "x2 = pd.read_csv(\"./Ressources/Datasets/stand_norm_e2.txt\", header=None, delimiter=\" \", names=[8, 9, 10, 11, 12, 13, 14])\n",
    "y = pd.read_csv(\"./Ressources/Datasets/y2_e1.txt\", header=None, delimiter=\" \", names=[\"label\"])\n",
    "x = pd.concat([x1, x2], axis=1)\n",
    "x = x[1:]\n",
    "\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test set.  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size = 0.25, random_state=0)\n",
    "\n",
    "# Feature Scaling\n",
    "# Adjust the mean to 0 and the standard deviation to 1\n",
    "st_x = StandardScaler()    \n",
    "x_train = st_x.fit_transform(x_train)    \n",
    "x_test = st_x.transform(x_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample to have same number of samples of each class\n",
    "smote = SMOTE()\n",
    "x_train_sampled, y_train_sampled = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction : CNN\n",
    "We are going to extract features from the datasets using the CNN method, knowing that it has the best performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, Input\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model = Sequential()\n",
    "# # Augmenter la taille de l'input si nécessaire ou ajuster les paramètres\n",
    "# cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', input_shape=(x_train_sampled.shape[1], 1)))\n",
    "# cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "# # Ajout d'une seconde couche convolutive avec padding pour conserver la dimension\n",
    "# cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "# cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(100, activation='relu'))\n",
    "# cnn_model.add(Dropout(0.5))\n",
    "# cnn_model.add(Dense(y_train_sampled.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compilation du modèle\n",
    "# cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Entraînement du modèle\n",
    "# cnn_model.fit(x_train_sampled, y_train_sampled, epochs=20, batch_size=20, verbose=1)\n",
    "\n",
    "# print_score(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction : LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "x_train_sampled_feat_extracted = lda.fit_transform(x_train_sampled, y_train_sampled)\n",
    "x_test_feat_extracted = lda.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(model):\n",
    "    score = model.score(x_train_sampled_feat_extracted, y_train_sampled.ravel())\n",
    "    print(\"Score : \" + str(score))\n",
    "\n",
    "\n",
    "def print_accuracy(y):\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y))\n",
    "\n",
    "\n",
    "def get_confusion_matrix_and_results(y):\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y)\n",
    "    cr = classification_report(y_test, y)\n",
    "\n",
    "    print(cr)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.9951163203693838\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators= 20, criterion=\"entropy\", class_weight=\"balanced\")  \n",
    "# random_forest.fit(x_train_sampled, y_train_sampled.ravel())\n",
    "random_forest.fit(x_train_sampled_feat_extracted, y_train_sampled.ravel())\n",
    "\n",
    "print_score(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.64      0.74      2511\n",
      "         1.0       0.32      0.49      0.39       682\n",
      "         2.0       0.00      0.10      0.01        10\n",
      "\n",
      "    accuracy                           0.61      3203\n",
      "   macro avg       0.39      0.41      0.38      3203\n",
      "weighted avg       0.74      0.61      0.66      3203\n",
      "\n",
      "[[1612  715  184]\n",
      " [ 249  334   99]\n",
      " [   5    4    1]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set result  \n",
    "y_pred = random_forest.predict(x_test_feat_extracted)\n",
    "\n",
    "get_confusion_matrix_and_results(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.5947877819215059\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "logistic_regression.fit(x_train_sampled_feat_extracted, y_train_sampled.ravel())\n",
    "\n",
    "print_score(logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.66      0.75      2511\n",
      "         1.0       0.34      0.40      0.37       682\n",
      "         2.0       0.01      0.30      0.01        10\n",
      "\n",
      "    accuracy                           0.60      3203\n",
      "   macro avg       0.41      0.45      0.38      3203\n",
      "weighted avg       0.76      0.60      0.67      3203\n",
      "\n",
      "[[1647  529  335]\n",
      " [ 217  276  189]\n",
      " [   5    2    3]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction on the test set\n",
    "y_pred = logistic_regression.predict(x_test_feat_extracted)\n",
    "\n",
    "get_confusion_matrix_and_results(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.7212306872669153\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20, weights=\"uniform\")\n",
    "\n",
    "knn.fit(x_train_sampled_feat_extracted, y_train_sampled.ravel())\n",
    "\n",
    "\n",
    "print_score(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.64      0.74      2511\n",
      "         1.0       0.36      0.51      0.42       682\n",
      "         2.0       0.00      0.10      0.00        10\n",
      "\n",
      "    accuracy                           0.61      3203\n",
      "   macro avg       0.42      0.42      0.39      3203\n",
      "weighted avg       0.77      0.61      0.67      3203\n",
      "\n",
      "[[1611  612  288]\n",
      " [ 207  345  130]\n",
      " [   5    4    1]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction on the test set\n",
    "y_pred = knn.predict(x_test_feat_extracted)\n",
    "\n",
    "\n",
    "get_confusion_matrix_and_results(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "\n",
    "decision_tree.fit(x_train_sampled_feat_extracted, y_train_sampled)\n",
    "\n",
    "print_score(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.58      0.69      2511\n",
      "         1.0       0.26      0.47      0.34       682\n",
      "         2.0       0.00      0.10      0.01        10\n",
      "\n",
      "    accuracy                           0.55      3203\n",
      "   macro avg       0.37      0.38      0.34      3203\n",
      "weighted avg       0.72      0.55      0.61      3203\n",
      "\n",
      "[[1449  893  169]\n",
      " [ 263  320   99]\n",
      " [   5    4    1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = decision_tree.predict(x_test_feat_extracted)\n",
    "\n",
    "\n",
    "get_confusion_matrix_and_results(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best classifier selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.9940507902681585\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('rf', random_forest), ('knn', knn), ('arbre decisionnel', decision_tree), ('logistic regression', logistic_regression)], voting='soft')\n",
    "voting_clf.fit(x_train_sampled_feat_extracted, y_train_sampled)\n",
    "\n",
    "print_score(voting_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.72      2511\n",
      "         1.0       0.30      0.48      0.37       682\n",
      "         2.0       0.00      0.10      0.01        10\n",
      "\n",
      "    accuracy                           0.59      3203\n",
      "   macro avg       0.39      0.40      0.37      3203\n",
      "weighted avg       0.74      0.59      0.65      3203\n",
      "\n",
      "[[1571  767  173]\n",
      " [ 251  328  103]\n",
      " [   6    3    1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = voting_clf.predict(x_test_feat_extracted)\n",
    "\n",
    "get_confusion_matrix_and_results(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "We are going to create a strong model with the Boosting technique using our previous simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    random_forest,\n",
    "    logistic_regression,\n",
    "    decision_tree\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_classifiers = []\n",
    "for base_model in base_models:\n",
    "    adaboost_classifier = AdaBoostClassifier(base_model, n_estimators=50, learning_rate=1)\n",
    "    adaboost_classifiers.append(adaboost_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model used : RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       n_estimators=20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elodie\\Desktop\\ESIEE\\IA\\PACMAN\\env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.62      0.72      2511\n",
      "         1.0       0.30      0.50      0.38       682\n",
      "         2.0       0.00      0.10      0.01        10\n",
      "\n",
      "    accuracy                           0.59      3203\n",
      "   macro avg       0.39      0.41      0.37      3203\n",
      "weighted avg       0.74      0.59      0.64      3203\n",
      "\n",
      "[[1546  779  186]\n",
      " [ 239  341  102]\n",
      " [   7    2    1]]\n",
      "\n",
      "Base model used : LogisticRegression(class_weight='balanced')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elodie\\Desktop\\ESIEE\\IA\\PACMAN\\env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.5812466702184337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.67      0.76      2511\n",
      "         1.0       0.33      0.36      0.34       682\n",
      "         2.0       0.01      0.30      0.01        10\n",
      "\n",
      "    accuracy                           0.60      3203\n",
      "   macro avg       0.41      0.44      0.37      3203\n",
      "weighted avg       0.76      0.60      0.67      3203\n",
      "\n",
      "[[1672  486  353]\n",
      " [ 219  243  220]\n",
      " [   5    2    3]]\n",
      "\n",
      "Base model used : DecisionTreeClassifier(class_weight='balanced')\n",
      "Score : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.57      0.68      2511\n",
      "         1.0       0.26      0.48      0.34       682\n",
      "         2.0       0.00      0.10      0.01        10\n",
      "\n",
      "    accuracy                           0.55      3203\n",
      "   macro avg       0.37      0.38      0.34      3203\n",
      "weighted avg       0.72      0.55      0.61      3203\n",
      "\n",
      "[[1436  908  167]\n",
      " [ 263  325   94]\n",
      " [   5    4    1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elodie\\Desktop\\ESIEE\\IA\\PACMAN\\env\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each AdaBoost classifier\n",
    "for i, adaboost_classifier in enumerate(adaboost_classifiers):\n",
    "    print(\"Base model used :\", base_models[i])\n",
    "\n",
    "    # Train the AdaBoost classifier\n",
    "    adaboost_classifier.fit(x_train_sampled_feat_extracted, y_train_sampled)\n",
    "\n",
    "    print_score(adaboost_classifier)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = adaboost_classifier.predict(x_test_feat_extracted)\n",
    "    \n",
    "    get_confusion_matrix_and_results(y_pred)\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESlib library\n",
    "We are using the DESlib library to select the best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deslib.des.des_clustering import DESClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# des = DESClustering(pool_classifiers=[knn, random_forest, decision_tree])\n",
    "# des.fit(x_train_sampled_feat_extracted, y_train_sampled.ravel())\n",
    "\n",
    "# print_score(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = des.predict(x_test_feat_extracted)\n",
    "\n",
    "\n",
    "# get_confusion_matrix_and_results(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
